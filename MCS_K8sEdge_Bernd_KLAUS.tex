% FH Technikum Wien
% !TEX encoding = UTF-8 Unicode
%

\documentclass[MSC,Master,english]{twbook}%\documentclass[Bachelor,BMR,ngerman]{twbook}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% BERND: Additional Pkg
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{graphicx}

\lstset{
    captionpos=b,
    numberbychapter=false,
    caption=\lstname,
    frame=single,
    numbers=none,
    stepnumber=1,
    numbersep=2pt,
    xleftmargin=15pt,
    framexleftmargin=15pt, 
    %umberstyle=\tiny,
    tabsize=3,
    columns=fixed,
    basicstyle={\fontfamily{pcr}\selectfont\footnotesize},
    keywordstyle=\bfseries,
    commentstyle={\color[gray]{0.33}\itshape},
    stringstyle=\color[gray]{0.25},
    breaklines,
    breakatwhitespace
}
\lstloadlanguages{bash}

% Define Code-Color
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% Bitte in der folgenden Zeile den Zitierstandard festlegen
%\newcommand{\FHTWCitationType}{IEEE}
%\ifthenelse{\equal{\FHTWCitationType}{HARVARD}}{\usepackage{harvard}}{\usepackage{bibgem}}
%BERND: TBH that package is 20 years old, get some more up2date packages for cite
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
%\usepackage[backend=biber,style=ieee,bibstyle=ieee]{biblatex}
%\usepackage[backend=bibtex,style=numeric,bibstyle=ieee]{biblatex}
%ARCH Linux fix
\usepackage[backend=biber,style=numeric,sortcites,natbib=true,sorting=none]{biblatex}
\addbibresource{Literatur.bib}

%Formatieren des Quellcodeverzeichnisses
\makeatletter
% Setzen der Bezeichnungen für das Quellcodeverzeichnis/Abkürzungsverzeichnis
%in Abhängigkeit von der eingestellten Sprache
\providecommand\listacroname{}
\@ifclasswith{twbook}{english}
{%
    \renewcommand\lstlistingname{Code}
    \renewcommand\lstlistlistingname{List of Code}
    \renewcommand\listacroname{List of Abbreviations}
}{%
    \renewcommand\lstlistingname{Quellcode}
    \renewcommand\lstlistlistingname{Quellcodeverzeichnis}
    \renewcommand\listacroname{Abkürzungsverzeichnis}
}
%Wenn die Option listof=entryprefix gewählt wurde, Definition des Entyprefixes für das
%Quellcodeverzeichnis. Definition des Macros listoflolentryname analog zu listoflofentryname und
%listoflotentryname der KOMA-Klasse
\@ifclasswith{scrbook}{listof=entryprefix}
{% 
    \newcommand\listoflolentryname\lstlistingname
}{%
}
\makeatother
\newcommand{\listofcode}{\phantomsection\lstlistoflistings}


%
% Einträge für Deckblatt, Kurzfassung, etc.
%
\title{Kubernetes on the Edge}
\author{Bernd KLAUS, BA}
\studentnumber{2010303012}
\supervisor{Dipl.-Ing. Hubert Kraut}
\secondsupervisor{Dipl.-Ing. Andreas Happe}
\place{Wien}

\kurzfassung{
Kubernetes wird als Schweizer Armemesser der Container-Orchestrierung bezeichnet. Auch im Bereich edge-computing bietet der Dienst eine Vielzahl an unterschiedlichen Werkzeugen und Tools an, welche Teils unterschiedliche Strategien und Ansätze verfolgen. Die Auswahl reicht von einem zentralen Kubernetes-Cluster der verteilte Geräte, sogenannte „Leafs“, steuert bis hin zu vielen einzelnen und verteilten kleinen Clustern an der Edge, welche zentral gesteuert werden. Entscheidend ist es den richtigen Anwendungsfall zu erheben, um sich für die optimale Lösung entscheiden zu können. Ebenfalls spielen sicherheitstechnische Aspekte bei derart komplexen Umgebungen eine wichtige Rolle. Die vorliegende Arbeit gibt Einblicke und Entscheidungsgrundlagen sowie Empfehlungen hinsichtlich der IT-Security. Belegt werden die Angaben durch Implementierung eines Proof-of-Concepts
}
\schlagworte{Kubernetes, edge-computing, distributed System, Proof-of-Concept}


\outline{
Kubernetes is the de facto swiss-army-knife for orchestrating container-platforms. In addition, Kubernetes can also be used for deploying devices as well as applications on top of it on the edge of the network. However, there are different methods for archiving comparable results. On the one hand a possible solution is to build a central instance managing small distributed and independent clusters, on the other hand a centralized cluster with just leafs on the edge may be a better fit. This results in the challenge to find the best solution for the desired environment respectively use-case. The following thesis is making use of "Design Science Research" to give introductions on how to choose the proper architecture for the aimed environment.
}
\keywords{Kubernetes, edge-computing, distributed System, Proof-of-Concept}

%
% Start the Thesis
%

\begin{document}
\maketitle
\chapter{Introdutcion}
\label{chap:introduction}
Because of \ac{IoT} Devices becoming more and more common, the number of devices capable of communicating with the \ac{WWW} increases rapidly. Consequently, also the overall traffic generated as well the amount of data which must be processed increases accordingly. Regarding this development edge-computing is the rising start trying to solve that issues. Thereby data is not processed centrally like in traditional datacenters, but it is tried to handle those data close to the user within several distributed systems. Because of this methodology only really necessary data is transmitted to a central instance for further treatment and those the processing-power as well as the bandwidth necessary for processing required data is reduced significant. \par It is expected that the number of IoT devices will continue to grow fast \cite{SotE21} over the coming years. Concomitant edge-computing also will become more important in the future and become an important role in modern \ac{IT} architectures. \par To be able to control distributed systems effectively \ac{K8s} is providing a lot of useful tools and functions. Fundamentally there are two different approaches regrading the architecture of how to build an edge-computing environment making use of \ac{K8s}:

\begin{itemize}
    \label{item:architecture}
    \item \textbf{Default}: A centralized \ac{K8s} Cluster controlling many leaf-devices (workers) on the Edge.
    \item \textbf{Distributed}: Small and distributed \ac{K8s} Clusters running independent on the Edge controlled by a centralized Master-Instance.
\end{itemize}

Another upcoming approach of solving that issue is making use of the service mesh \cite{servicemesh}. This ultimately uses or builds on both of the aforementioned technologies. However, since this thesis concentrates on the two main architectures and their differentiation, the service mesh is not the main focus and just mentioned for the sake of completeness.

\section{Problem area}
Problems arise when trying to find the proper architecture for a specific use-case. There is no clear winner when comparing the above-mentioned different variants. Each of them  has their own pros and cons and may decide whether a project is successful or not. It is therefore all the more important to choose the proper architecture right before starting, changing the strategy in retrospect would take a lot of time and effort. However, there is no clear guidance on how to find the proper target environment, at least none which apply in general. Occasionally one finds recommendations for a very specific use case, however the chance is slim low this findings fit your goals respectively enlighten the decisions. This leads us to the following research question.

\section{Research question}
\label{sec:rq}
This paper is going to answer the subsequent research questions:
\begin{enumerate}
    \item What are the main differences of the in \autoref{item:architecture} mentioned architectures regarding functionality, scalability, costs and security?
    \item Which decision criteria must be defined respectively examined to create a catalog capable
    of choosing the proper architecture easier for \ac{IT} managers as well as administrators?
    \item Is there a trend in which technology is most likely to be used?
\end{enumerate}

\section{Goal}
\label{sec:goal}
The main goal of this thesis is to highlight the pros and cons for each of the \hyperref[item:architecture]{architectures} defined in the \hyperref[chap:introduction]{Introduction}. The focus will be mainly on the geo-distribution aspect. Although \ac{IoT} is playing a major role in pushing the development forward, however it is not considered further in the present work. To find the proper architecture, or at least recommendations what could fit best for different desired use-cases, a catalog will be defined. An important part will become the decision tree helping people making comprehensible decisions based on scientific research. The main characteristics which are taken into account are scalability, state-of-the-art, handling, costs as well as security.

\section{Methodology}
\label{sec:methodology}
In the first part of the present work existing literature will be inspected. Related and relevant work will be examined accordingly and linked in the document. Also results will be incorporated to get out the most of it. The goal is to create a catalog with main criteria necessary for decision-making. Part of this catalog will also be a decision-tree, mentioned in the previous chapter, to easily find the proper architecture. The \ac{DSR} method serves as a scientific method and to test the characteristics recorded in the catalogue. This chapter is given the most attention, it is the area where new techniques or architectural decisions are finally verified and the proof is given whether the catalogue works as expected or not. In the latter case, the catalog will be revised to reflect the findings of the last step and re-examined again using \ac{DSR}.


\chapter{State of the Art}
\label{chap:current}
\section{Technology}
The present chapter provides an introduction to the general thematic. The main components and objects of \ac{K8s} are explained aswell as the layers of edge-computing are highlighted. If anyone is already familiar with the subjects, may you jump over to the \autoref{sec:architecture} "architecture" to read further.
\label{sec:technology}
\subsection{Kubernetes}
To promote modern development and be able to implement continuous deployment pipelines cumbersome monolithic applications are divided into many smaller units. Each of these units provides only one function. In order to establish the overall functionality, these units are communicating with each other and thus provide services or make use of other ones. This new method of delivering applications brings many advantages in terms of development but also introduce some new challenges and complexities regarding operation. To simplify the tasks around the management of this architecture, \ac{K8s} has established itself as the de facto standard \cite{k8ssurv}. \ac{K8s} was initially developed by Google and later donated to the opensource community. Over the course of time, a broad community has developed around \ac{K8s} and a number of additional tools and extensions have emerged as a result. The most promising solutions regrading geo-distribution respectively edge-computing are highlighted in the subsequent \autoref{sec:architecture} "architecture". In order to be able to interpret the results of the use-cases, as well as building the necessary basic understanding, the following functionalities and components of \ac{K8s} are of relevance.  

\begin{figure}[ht]
    \centering
    \tcbox[sharp corners, boxsep=2mm, boxrule=0.2mm, colframe=gray!20!gray, colback=white]{\includegraphics[width=0.75\textwidth]{PICs/k8s-architecture.png}}
    \caption{\ac{K8s} architeture overview \cite{pic-k8s-overview}}
    \label{fig:k8s-architecture}
\end{figure}

\paragraph{Master Nodes} run the so-called \textit{Control Plane} which is responsible for controlling the cluster itself and all the ressources within. The Control Plane consist of the following components \cite{k8scomp}.
\begin{itemize}
    \item \textit{kube-apiserver} acts as frontend web-interface responsible for controlling the \ac{K8s} cluster as well as the objects inside the cluster. Tools like kubectl abstract the \textit{OpenAPI v2} endpoint and provide access in form of a simply understandable and usable \ac{CLI}. 
    \item \textit{etcd} represents a high-available and consistent key value store responsible for storing the actual state as well as the desired configuration of the cluster.
    \item \textit{kube-scheduler} is responsible for scheduling pods on the available worker nodes. Decision variables such as available ressources, affinity-rules and constraints are taken into account. However, the default \textit{kube-scheduler} ist not aware of any latency between the worker nodes nor the pods communicating with each other. As discovered in the following chapters, this appears to be an important variable for edge-deployments. However, some available white-papers already try to address those issues and show possible solutions by adopting a custom scheduler taking care of those values. More details on this can be found in the \autoref{chap:related} "related work".
    \item \textit{kube-controller-manager} consists of a single compiled binary controling the status of nodes, jobs, service-accounts and endpoints as well as creating or removing them.
    \item \textit{cloud-controller-manager} represents the interface to the underlying cloud-platform. This allows kubernetes to create and/or configure load-balancers, routes and persistent-volumes on the underlying cloud-infrastructure. In a local environment e.g. minikube \cite{minikube} provide the \textit{cloud-controller-manager} becomes an optional component and is not required. The same may apply to edge-locations as those areas are outside the cloud most of the time.
\end{itemize}

\paragraph{Worker Nodes} manage the workload, i.e. run the actual application(s). These nodes are composed of the following, see list below, parts \cite{k8scomp}. It should be mentioned, that also the described \textit{Master Nodes} are executing those components because some core-componentes are containerized (pods) itself. 
\begin{itemize}
    \item \textit{kubelet} is an agent which assures that the container is executed properly inside their associated pods according to its specifications defined via \textit{PodSpec}. Also \textit{kubelet} is responsible for monitoring the healthy state of the containers.
    \item \textit{kube-proxy} uses the packet filters of the operating system underneath to forward traffic to the desired destination. The resulting access points, also called \textit{Services} in \ac{K8s}-jargon, can be made available either internally or externally.
    \item \textit{container runtime} is the part that finally executes the containers. The default runtime at time of writing is \textit{containerd}, however any runtime is supported that complies with the CRI specification \cite{cri-runtime}.
\end{itemize}

\paragraph{Kubernetes Objects} are persistent properties inside the \ac{K8s} ecosystem representing the state of a cluster. The most important feature of those objects is to describe the target environment in a declarative way. For this purpose, most of the time, YAML files are used. Kubernetes now ensures that the desired state of the environment is actually achieved and continuously monitors the required objects to meet those defined requirements. This mechanism is also ideal for distributed systems, such as edge computing, as availability can be monitored at any time and a an action can be taken if necessary. Subsequent the main objects are cited starting with the smallest unit \cite{k8sconc}.
\begin{itemize}
    \item \textit{Containers} decouple the actual application and its dependencies from the underlying infrastructure. The main properties of those containers are there immutability and repeatability. This means that the container can be rebuilt at anytime resulting in an identical clone. Likewise, the code of a running container cannot be modified subsequently.
    \item \textit{Pods} include at least one or more \textit{Containers}. In the most scenarios a single pod consists of a single container, in some cases a so-called sidecar container is used increasing the number of containers inside a pod. Containers which are in the same \textit{Pod} share the same local Socks as well as volumes mounted.
    \item \textit{Deployments}, \textit{Statefulsets} and \textit{Daemonsets} are responsible for ensuring the actual workload is provided, to achieve this they control and scale the assigned \textit{Pods}. When creating an application for \ac{K8s}, it is most likely to create one of those objects. The \textit{Pods} and \textit{Containers} are merely an end product that is derived these objects.
    \item \textit{Services} provide an abstract way to make a set of \textit{Pods} available on the network via a single endpoint. Additional deployed pods will automatically be added to the responsible \textit{Services}. Thereby \ac{K8s} is an excellent choice when it comes to scaling applications without any manual intervention. This also applies for deploying applications to the edge of the network, as illuminated in the course of this thesis. Closely related to the \textit{Services} is the \textit{Ingress} resource, which is taking care of making the aforementioned objects available outside the cluster. An optional reverse-proxy (\textit{Ingress-Controller}) must be installed in order to make use of the latter. \newline
    A new feature, which is of relevance regarding edge-computing, currently in beta phase, is the so-called \textit{Topology Aware Hint}. Basically its meta-data added to the endpoints defined previously suggesting the connection client on how to reach the destination efficiently (e.g. zones aware of different locations can be defined)
    \item \textit{ConfigMaps} and \textit{Secrets} are pieces of information which can be mounted into to \textit{Container} to adjust the configuration inside at runtime. Even whole files can be replaced using on of them. \textit{Secrets} are only different in the sense that they decode the content, however technically they are the same.
    \item \textit{Volumes} provide persistent storage which extends beyond the life cycle of the pods. Volumes can be mounted at any defined position inside the pods. The disadvantage is that the data written to those \textit{Volumes} resides out of the \ac{K8s} ecosystem and therefore the operator must take care of data security and replication. This becomes even more complicated in an edge-computing environment where nodes have higher latency between them.
\end{itemize}

\subsection{Edge-Computing}
Edge-computing is the model that extends cloud services to the edge of the
network. The computing resources on the edge act as a layer between the user, who provides or wants to process data, and the centralized datacenter (e.g. the cloud). Because data can be processed earlier respective closer to the user, latency and amount of data transferred can be reduced \cite{intro-edge}. Also, the required computing-ressources in the datacenter can be minimized because data can be processed at the edge. A major driver of the subsequent s technology is \ac{IoT}. The amount of devices and resulting data volume, which must be processed, is increasing exponentially \cite{SotE21}. Another technology which depends on it are low-latency applications like e.g. video-streaming. 

\paragraph{Hierarchy} descripes the layers of the architecture. The following list enumerates the most important layers from top to bottom \cite{intro-edge}.
\begin{enumerate}
    \item \textit{Cloud} - centralized datacenter
    \item \textit{Fog} - distributed "smaller" datacenters
    \item \textit{Edge} - the closest unit to the end-user
    \item \textit{\ac{IoT}} - device at the edge put into use
\end{enumerate}


The main focus of this work is to efficiently combine the two layers \textit{Cloud} and \textit{Edge} and orchestrate between them using \ac{K8s}. The layer \textit{Fog} is skipped because it is often seen to be "the same" as the \textit{Edge}. Also, current \ac{K8s} based solutions do not make use of it. 

\paragraph{Geo-Distribution} characterises the aspect of the geographical propagation of the edge ressources. The goal is to provide computing power over wide areas, each close to the users. By establishing many of these locations in different regions, network latency can be significantly reduced from the user's perspective. However, the latency between the edge-nodes and the centralized cloud still remain.


\section{Architecture}
\label{sec:architecture}
This chapter focuses on the two different architecture approaches which can be used for edge computing. After an overview the advantages and disadvantages as well as possible solutions are examinated.

\subsection{Default}
In order to be able to manage resources at the edge, a traditional architecture can be used. This is subdivided into a centralized \textit{Control Plane} hosted in the cloud and distributed worker nodes near the edge. The same \ac{K8s} architeture is commonly used when deploying to a single location aswell inside the cloud. The following graphic illustrates the architecture.

\begin{figure}[ht]
    \centering
    \tcbox[sharp corners, boxsep=2mm, boxrule=0.2mm, colframe=gray!20!gray, colback=white]{\includegraphics[width=0.70\textwidth]{PICs/drawio/defaul-k8s.drawio.pdf}}
    \caption{Default \ac{K8s} architeture}
    \label{fig:k8s-default}
\end{figure}

Although the conventional components can also communicate with each other over long distances, there are still some challenges that need to be taken into account. An important role in this context is played by the \ac{CNI}. This part of \ac{K8s}, which can be selected in the form of a plugin, is responsible for the cluster-internal network traffic. Most of these plugin providers offer advanced features that facilitate geographical distribution. Some of the most used \ac{CNI} providers are \cite{k8s-cni}:

\begin{itemize}
    \item Calico
    \item Canal
    \item Cilium
    \item Flannel
    \item Weave
\end{itemize}

Other issues, which must be observed, are the replication of the storage and metadata hold by the control planes. Because of the higher latency and-or unstable connection at the edge site data may cannot always be reliably retrieved. To circumvent this problem, asynchronous replication can be used for storage replication. Another solution is to make services at the edge stateless, thereby no big data chunks are requested at all. The important part of the metadata store are \ac{DNS} entries, because \ac{K8s} heavily relies on them for service discovery. NodeLocal DNS is the recommand way \cite{k8sdnslocal} to hold a copy on the worker nodes. In contrast to this forwarding data to the cloud is not an issue in most of the times when using suitable message queues.  

In addition there is no awarness of where workloads are running and the level of latency between nodes when using vanilla kubernetes. There are some developments in this area, but they have not really caught on yet \cite{k8s-sharping-edge}\cite{tk-k8s-edge-scheduler}\cite{5g-k8s-scheduler}.

\paragraph{KubeEdge} is an open-source \ac{CNCF} project \cite{hal-kubeedge} that already comes with many of these functions respectively requirements pre-charged. Likewise, this tool was explicit developt for edge-computing.  Worker nodes can therefore be distributed across the hole globe without any major adjustments. To make this possible, some components were added or exchanged \cite{kubedge}.

\begin{itemize}
    \item \textit{KubeBus} - a custom network plugin working in private \ac{IP} address ranges behind \ac{NAT}. Thereby no dedicated public ip is necessary for edge locations making the architecture more flexible.
    \item \textit{SyncService} - Another important part of KubeEdge is the metadata sync service running on each worker node. This extension cyclically synchronises the data of the master. Thus, the quantity of data to be called up can be minimized and workload can continue to run in offline scenarios.
    \item \textit{EdgeCore} - Kubelet is replaced with a lightweight custom agent, the EdgeCore. Thereby also devices with very limited ressources, e.g. a Raspberry Pi, can be used for running workloads.
\end{itemize}

In addition a component for device management and seemles integration with \ac{MQTT} is built-in to KubeEdge\cite{hal-kubeedge}. However, because this thesis focuses on the geo-distribution aspect, this part is not researched further.


\subsection{Distributed K8s}
\label{sec:disk8s}
In this case, a different approach is chosen and, in contrast to the previous architecture, fully-fledged clusters are also operated at the edge. The obvious advantage is that they can be operated autonomously. There is basically no dependence on the other nodes.

\begin{figure}[ht]
    \centering
    \tcbox[sharp corners, boxsep=2mm, boxrule=0.2mm, colframe=gray!20!gray, colback=white]{\includegraphics[width=0.70\textwidth]{PICs/drawio/distributed-k8s.drawio.pdf}}
    \caption{Distributed \ac{K8s} architeture}
    \label{fig:distributed-k8s}
\end{figure}

However, this also creates new challenges such as the distribution and coordination of workloads. Without additional functionality added, the workload running on the clusters does not know about their neighbours. To overcome this gap, the division  "multi-cluster deployment" has excelled in recent years. Even Google, the original developer of \ac{K8s}, offers such a service \cite{google-mcs} in their public cloud. Due to the increased demand, the \ac{KubeFed} tool is officially being maintained and further developed by the \ac{K8s} project\cite{kubefed-github}. Although the tool is still in beta, it is a very good choice for our field of application. One possible alternative would the \ac{K8s} manager by Rancher Labs.

\paragraph{\ac{KubeFed}} consists of a central hosting cluster which controls subordinate clusters via \ac{API} delegation. Within \ac{KubeFed} you can define which configuration should be managed by the hosting cluster. The methodes used are intentionally low-level and can thus be expanded well for different edge-deployment variants. Two different types of information is used to configure \ac{KubeFed}\cite{kubefed-github}:

\begin{itemize}
    \item \textit{Type configuration} - determines which \ac{API} types are handeld.
    \item \textit{Cluster configuration} - determines which clusters should be targeted.
\end{itemize}

The type configuration itself consist of three main parts:

\begin{itemize}
    \item \textit{Temaplates} - defines a reprenentation of common ressources across the clusters.
    \item \textit{Placement} - defines which destination-cluster the workload should run on.
    \item \textit{Overrides} - defines varation of the templates on a per cluster level.
\end{itemize}

\section{General Challenges} 
\label{sec:generalchallanges}
Challenges that appear when creating respectively operating such infrastructures are diverse, as can be seen in the following list \cite{intro-edge}.

\begin{itemize}
    \item \textit{Variety} - a lot of different locations, technologies as well as methods on how to control devices on the edge is challenging for both development and operation. The better these different factors can be abstracted and simplified, the more effectively the infrastructure can be used.
    \item \textit{Integration} - edge-computing evolves very quickly, thereby things could change quickly. The more important it is to keep the provided interfaces extensible. This way, new devices or application can be put in use swiftly. 
    \item \textit{Awareness} - the devices and/or end-user do not care about how their traffic is routed, or their data is processed. However, the architecture needs to take care of that to use the topology in the best possible way.
    \item \textit{Resources} - scaling Resources like \ac{CPU}, \ac{RAM} and disk space at the edge is by far more elaborate than in a datacenter or in the cloud.
    \item \textit{\ac{QoS}} - The service provided at the edge should be reliable and provide a good user experience. Availability and performance play a central role in this context. As the availability can not be guaranteed to some extent, an appropriate failover mechanism should be in place.
    \item \textit{Security} - physical access control as well as isolating applications from each other is a difficult task. Also, the data traffic must be separated accordingly. In general, \ac{IT} security is a hot topic, and especially at the edge, it requires appropriate consideration for hardening the environment.
    \item \textit{Monitoring} - another important factor is how to capture metrics and events (logs) from the edge. They need to be indexed on a centralized instance in order to get a general overview on what's happening. Because of the dynamic and rapid changes some kind of automatic discovery should be used for that purpose.
    \item \textit{Environment} - Some locations may have to deal with difficult conditions regarding their surroundings. Increased dust exposure, poor internet connections or recurring power outages can be some of these factors. The system must be able to cushion or parry such failures accordingly.
\end{itemize}
The above-mentioned challenges provide a good starting point for defining the necessary tests to find the matching target architecture. Details about this test can be found in the \autoref{sec:dsrmethode} "methodlogy".



\chapter{Design Science Research}
\label{chap:dsr}
"A common topic when performing research in technical disciplines is to design some kind of artefact, such as a model, an information system, or a method for performing a certain task. To address this topic in a systematic and scientific way, Design Science Research (DSR) has stablished itself as an appropriate research method." \cite{dsr-method}

\section{Methodology}
\label{sec:dsrmethode}
According to the description in the introduction before, \ac{DSR} is used as a scientific method to construct the \ac{PoC}. For this purpose two real world exmaples are build, which are then gradually refined. Those exmaples are descripte in detail in the following \autoref{sec:dsrenv} "environment". The same \hyperref[sec:dsrusecase] are then deployed onto each of these environments. Afterwards, Tests are then defined and carried out on the basis of the issues listed in section \autoref{sec:generalchallanges} "genereal challenges". Based on the results, a preferred environment per property can finally be determined.

\section{Environment}
\label{sec:dsrenv}
\subsection{Generel}
The main goal is to create both of the aimed architectures in a similar environment. The target environment should provide good coverage of the diversity encountered in edge environments. For that poropse the \ac{PoC} is build across \hyperref{https://www.hetzner.com/cloud}{}{}{Hetzner Cloud} and a local site connected via 4G mobile internet.

\paragraph{Coverage} This combination allows a wide range of possibilities to be achieved:

\begin{itemize}
    \item \textit{Geographical distribution} - Hetzner Cloud offers location in Germany, Finland and the U.S for deployments. This allows communication to take place over long distances.
    \item \textit{Scalability} - On the Hetzner Cloud scaling nodes can be realised quickly via \ac{API}.
    \item \textit{\ac{NAT}} - At the local site, nearby vienna, no public IP is availabe for each of the nodes. Therefore, \ac{NAT} is used to map a dynamic changing IP to the nodes downstream.
    \item \textit{\ac{ARM}} - Also at the local site, a Raspberry Pi is used as an edge-node to emulate devices with minial ressources.
    \item \textit{Unstable} - The connection at the local site may be unstable from time to time because the connection is established over the public 4G network using mobile technology.
\end{itemize}

\paragraph{Locations} Subsequent, a tabular lists all of the nodes that are used, followed by a graphic showing the locations on a map.

\begin{table}[ht]
    \begin{center}
        \begin{tabular}{|l|l|l|l|l|l|l|}
            \hline
            Node & Location & IP & CPU & Cores & RAM & Latency \\
            \hline
            Master & Nürnberg, DE & dedicated & AMD64 & 2 & 4GB & 0ms \\
            Edge-1 & Ashburn, US & dedicated & AMD64 & 2 & 2GB & 95ms \\
            Edge-2 & Helsinki, FI &  dedicated & AMD64 & 2 & 2GB & 24ms \\
            Raspberry & Kirchberg, AT & dynamic & ARM64 & 4 & 4GB & 40ms \\
            \hline
        \end{tabular}
        \caption{\ac{PoC} nodes specification}
        \label{tab:poc-nodes}
    \end{center}
\end{table}

\begin{figure}[ht]
    \centering
    \tcbox[sharp corners, boxsep=2mm, boxrule=0.2mm, colframe=gray!20!gray, colback=white]{\includegraphics[width=0.80\textwidth]{PICs/poc-map.png}}
    \caption{Map of the \ac{PoC}\cite{googlemaps}}
    \label{fig:poc-map}
\end{figure}

\paragraph{OS} The \ac{OS} used on all cloud nodes is Ubuntu 20.04.3 LTS. For the local site on the Raspberry Pi the \ac{OS} Raspberry Pi Lite, without \ac{GUI}, is used.

\paragraph{K3s} For simplicity, the tool of choice for installing \ac{K8s} is \hyperref{https://k3s.io/}{}{}{K3s}. This is a lightweight \ac{K8s} distribution optimized for \ac{IoT} and deployments at the edge. All necessary features are supported, only the dependencies have been reduced to the essentials. In actual use, insofar as resources in the cloud only play a subordinate role, the standard \ac{K8s} can also be used analogously. However, at the edge K3s if a perfect match. To illustrate the simplicity, the installation command used is shown below.  

\begin{lstlisting}[caption={K3s installation},captionpos=b]
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--disable traefik --disable-cloud-controller" sh -s -
\end{lstlisting}

\paragraph{VPN} No \ac{VPN} is used for either environment. Although a VPN can increase security accordingly and services can communicate more easily, its use in a widely distributed edge environment with many nodes is problematic. VPNs are simply not designed for unstable connections with high latency and possibly rapidly changing \ac{IP} addresses. The two tools used therefore use alternative approaches via HTTPs and RPC respectively.

\subsection{KubeEdge}
\label{sec:dsrenvke}
For the KubeEdge variant, only a \ac{K8s} installation is required on the master. Edge nodes just need be to equipped with a supported containter runtime of choice. In our specific \ac{PoC} Docker is used. KubeEdge takes over the roles of the kublete and the k-proxy as shown in the figure \hyperref[fig:k8s-default]{"default K8s architecture"}. A single binary (keadm) is used to initialise KubeEdge. A token and the public IP are specified as parameters on the master. On the edge nodes, those information is used to join the master. 

\paragraph{CloudStream} While the standard installation can be done easily, activating logs is much more challenging. Activities such as generating certificates, setting environment vairables and adapting configuration files must be carried out manually.

\paragraph{Raspberry} On the \ac{ARM} devices additional steps where required to configure cgroups appropriately. Otherwise the installation steps are exactly the same as on the cloud nodes.

\paragraph{Installation} Steps for installation can be founf in the Github repository: \hyperref{https://github.com/Berndinox/K8sEdge/blob/main/DOCs/kubeedge-install.md}{}{}{Berndinox/K8sEdge}

\subsection{KubeFed}
\label{sec:dsrenvkf}
In contrast to KubeEdge, as also noted in \autoref{sec:disk8s}, KubeFed relies on individually acting clusters that are controlled by a central instance. Because of this, each node must be equipped with a fully functional \ac{K8s} cluster. Usually, the installation is associated with greater effort, but K3s simplifies this process enormously, as descripted in \autoref{sec:dsrenv}.

\paragraph{Hosting Cluster} The challenges arise when it comes to connecting all instances from the central cluster. The kubeconfig for each of them must be modified, to include the puclic ip of the node, and transmitted to the central hosting cluster. There, the configuration must be added as an additional context. Finally, a custom binary is used to add each of the defined contexts to \ac{KubeFed}, as shown below.

\begin{lstlisting}[caption={KubeFed join context},captionpos=b]
kubefedctl join edge1 --cluster-context edge1 --host-cluster-context default --v=2
\end{lstlisting}

\paragraph{Installation} Steps for installation can be founf in the Github repository: \hyperref{https://github.com/Berndinox/K8sEdge/blob/main/DOCs/kubefed-install.md}{}{}{Berndinox/K8sEdge}

\section{Use-Cases}
\label{sec:dsrusecase}
\subsection{Web-Application}
\subsection{Enterprise VPN}
\subsection{Distributed Database}

\section{Performed Tests}

\section{Analysis}
\label{sec:dsranalysis}
\subsection{Relevant Magnitudes}
\subsection{Outcome}
\subsection{Paraphrase}



\chapter{Catalog}
\label{chap:catalog}

\section{Decision Variables}
\label{sec:variables}
\subsection{Installation complexity}
The first decision variable that is examined is the effort required for the installation and the associated complexity. According to the KISS principle\cite{kiss}, those solutions with less complexity should be preferred. One investigates the installation routine, descripted in the  \autoref{sec:dsrenv} "environment", very different steps and necessary tools were discovered.

\section{Decision Tree}
\label{sec:tree}


\section{Exclusions and Special Cases}
\label{sec:exclusions}




\chapter{Related Work}
\label{chap:related}
\section{Kubernetes and the Edge?}
Some introdution to K8s at the Edge, highlighting the main Architectures.
\section{Extend Cloud to Edge with KubeEdge}
Descripes KubeEdge and its advantages
\section{Sharpening Kubernetes for the Edge}
Sharpening Kubernetes for the Edge
Make Kubernetes aware of the latency between the nodes at the Edge.
\section{Ultra-Reliable and Low-Latency Computing in the Edge with Kubernetes}
Similar to the paper before. Latecny awar pod deploymentm, but you also can deploy to regions and an custom re-scheduler is implementated taking care of redeploying when one node fails.
Clustering node-groups based on latency.


\chapter{Results}
\label{chap:results}

\section{Findings}
\label{sec:findings}

\section{Conclusio}
\label{sec:conclusio}

\section{Discussion and further research}
\label{sec:discuss}




%
% Hier beginnen die Verzeichnisse.
%
\clearpage
%\ifthenelse{\equal{\FHTWCitationType}{HARVARD}}{}{\bibliographystyle{biblatex}}
%\bibliography{Literatur}
\printbibliography 
\clearpage

% Das Abbildungsverzeichnis
\listoffigures
\clearpage

% Das Tabellenverzeichnis
\listoftables
\clearpage

% Das Quellcodeverzeichnis
\listofcode
\clearpage

\phantomsection
\addcontentsline{toc}{chapter}{\listacroname}
\chapter*{\listacroname}
\begin{acronym}[XXXXX]
    \acro{IT}[IT]{information technology}
    \acro{WWW}[WWW]{world wide web}
    \acro{K8s}[K8s]{Kubernetes}
    \acro{IoT}[IoT]{internet-of-things}
    \acro{DSR}[DSR]{Design Science Research}
    \acro{CLI}[CLI]{Command Line Interface}
    \acro{QoS}[QoS]{Quality of Service}
    \acro{CPU}[CPU]{Central Processing Unit}
    \acro{RAM}[RAM]{Random Access Memory}
    \acro{CNI}[CNI]{Container Network Interface}
    \acro{IP}[IP]{Internet Procotcol}
    \acro{NAT}[NAT]{Network Address Translation}
    \acro{DNS}[DNS]{Domain Name System}
    \acro{MQTT}[MQTT]{Message Queuing Telemetry Transport}
    \acro{CNCF}[CNCF]{Cloud Native Computin Foundation}
    \acro{KubeFed}[KubeFed]{Kubernetes Cluster Federation}
    \acro{API}[API]{Application Programming Interface}
    \acro{PoC}[PoC]{Proof-of-Concept}
    \acro{ARM}[ARM]{Advanced RISC Machines}
    \acro{OS}[OS]{Operating System}
    \acro{GUI}[GUI]{Graphical User Interface}
    \acro{VPN}[VPN]{Virtual Private Network}
\end{acronym}

%
% Hier beginnt der Anhang.
%
\clearpage
\appendix
\chapter{Appendix}
\clearpage
\chapter{Appendix}
\end{document}